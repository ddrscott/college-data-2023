{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and required libraries\n",
    "\n",
    "```sh\n",
    "pip install requests\n",
    "export LITELLM_API_KEY='sk-123abcXYZ'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "\n",
    "LITELLM_API_KEY = os.getenv('LITELLM_API_KEY', 'sk-123abcXYZ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "   qwen2.5:14b\n",
      "   codestral:latest\n",
      "   llama3.2-vision:latest\n",
      "   llama-3.1-70b-versatile\n",
      "   llama3.2:latest\n",
      "   smollm2:latest\n",
      "   qwen2.5-coder:latest\n",
      "   llava-phi3:latest\n",
      "   llama3.1:70b-instruct-q2_k\n",
      "   llama3.1:latest\n",
      "   qwen2.5-coder:14b\n",
      "   minicpm-v:latest\n"
     ]
    }
   ],
   "source": [
    "# Fetch available Models\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://litellm.dataturd.com/v1/models\"\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': f'Bearer {LITELLM_API_KEY}'\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "response.raise_for_status()\n",
    "\n",
    "print('Available models:')\n",
    "for model in response.json()['data']:\n",
    "    print('  ', model['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-fcf47111-c240-4039-84b6-d1931d229dc4\",\n",
      "  \"created\": 1734541984,\n",
      "  \"model\": \"ollama/qwen2.5:14b\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"system_fingerprint\": null,\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"Function calls itself,  \\nDeep into mirrors it vanishes\\u2014  \\nLoop of reflection.\",\n",
      "        \"role\": \"assistant\",\n",
      "        \"tool_calls\": null,\n",
      "        \"function_call\": null\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 18,\n",
      "    \"prompt_tokens\": 50,\n",
      "    \"total_tokens\": 68,\n",
      "    \"completion_tokens_details\": null,\n",
      "    \"prompt_tokens_details\": null\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Make a Chat Completion Request\n",
    "import requests\n",
    "\n",
    "url = \"https://litellm.dataturd.com/v1/chat/completions\"\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': f'Bearer {LITELLM_API_KEY}'\n",
    "}\n",
    "data = {\n",
    "    \"model\": \"qwen2.5:14b\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a haiku about recursion in programming.\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "response.raise_for_status()\n",
    "\n",
    "print(json.dumps(response.json(), indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function calls itself,\n",
      "Deep into the mirror maze,\n",
      "Endless loop of code."
     ]
    }
   ],
   "source": [
    "# Make a streaming Chat Completion Request so you can print each word as it comes in\n",
    "import requests\n",
    "\n",
    "url = \"https://litellm.dataturd.com/v1/chat/completions\"\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {LITELLM_API_KEY}'\n",
    "}\n",
    "\n",
    "# The data you want to send in the request body\n",
    "data = {\n",
    "    \"model\": \"qwen2.5:14b\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Write a haiku about recursion in programming.\"}\n",
    "    ],\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data, stream=True)\n",
    "\n",
    "for line in response.iter_lines():\n",
    "    decoded = line.decode('utf-8')\n",
    "    if decoded.startswith('data: {'):\n",
    "        data = json.loads(decoded[6:])\n",
    "        if choices := data.get('choices', []):\n",
    "            if delta := choices[0].get('delta', {}):\n",
    "                if content := delta.get('content', False):\n",
    "                    print(content, end='')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment Reviewer Full Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"professionalism\": 0.8,\n",
      "    \"emo_ism\": 0.9,\n",
      "    \"accuracy\": 0.7,\n",
      "    \"overall\": 0.85,\n",
      "    \"reason\": \"The comment showcases a unique perspective on the interview processes of Google and Amazon, which aligns with the user's interest in dark and depressing topics. The use of probability assignments to describe the likelihood of a 'good bullshitter' or 'competent' hire adds an element of pain and awkwardness that resonates with the user's preference for uncomfortable situations.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from textwrap import dedent\n",
    "\n",
    "# LITELLM_API_KEY = os.getenv('LITELLM_API_KEY', None)\n",
    "if not LITELLM_API_KEY:\n",
    "    raise Exception('Please set the LITELLM_API_KEY environment variable')\n",
    "\n",
    "def reply(messages, model='llama3.2:latest'):\n",
    "    url = \"https://litellm.dataturd.com/v1/chat/completions\"\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "    'Authorization': f'Bearer {LITELLM_API_KEY}'\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Request failed with status {response.status_code}: {response.text}\")\n",
    "    \n",
    "    return response.json()['choices'][0]['message']['content']\n",
    "\n",
    "def review_comment(post:str, comment:str):\n",
    "    return reply([\n",
    "        {\"role\": \"system\", \"content\": dedent(f\"\"\"\n",
    "            You are an unbiased content reviewer.\n",
    "            Given the following LinkedIn post:\n",
    "            ---\n",
    "            {post}\n",
    "            \"\"\")},\n",
    "        {\"role\": \"user\", \"content\": dedent(f\"\"\"\n",
    "            My interests are:\n",
    "            I'm EMO and like dark depressing things. I don't like popular opinion, and looking for awkward painful situations are more interesting to me. Doom scrolling is good for me.\n",
    "\n",
    "            Evaluate the quality of this user submitted comment based on my interests:\n",
    "            ```txt\n",
    "            {comment}\n",
    "            ```\n",
    "            Format the response in JSON format:\n",
    "            ```json\n",
    "            {{\n",
    "            professionalism: 0-1,\n",
    "            emo_ism: 0-1,\n",
    "            accuracy: 0-1,\n",
    "            overall: 0-1 // 0 is poor, 1 is great,\n",
    "            reason: \"the reason for the score\"\n",
    "            }}\n",
    "            ```\n",
    "            Only give the response.\n",
    "            \"\"\")}\n",
    "    ])\n",
    "\n",
    "def strip_code_fence(text):\n",
    "    return '\\n'.join([line for line in text.split('\\n') if not line.startswith('```')])\n",
    "\n",
    "\n",
    "post = dedent(\"\"\"\n",
    "    Carlos ArguellesCarlos Arguelles\n",
    "    • 3rd+ • 3rd+\n",
    "    Senior Principal Engineer at Amazon, ex-Google, ex-MicrosoftSenior Principal Engineer at Amazon, ex-Google, ex-Microsoft\n",
    "    1d • Edited •  1 day ago\n",
    "\n",
    "    Follow\n",
    "    Different interview processes can yield very different personality types into tech companies. After having participated in the Amazon interview process (800+ interviews in 12 years) and the Google interview process (50 interviews in 4 years), it is my personal opinion that a number of googlers would not make it through an amazon interview loop (and vice versa). Before you throw rotten tomatoes at me, there’s plenty of xooglers at amazon and plenty of ex-amzn at google, so bear with me as I elaborate.\n",
    "\n",
    "    Amazon takes a very holistic approach to interviewing, placing roughly equal value in Leadership Principles and Technical skills + raw IQ. Google places a somewhat myopic focus on Tech+IQ above all else. The blend of tech skills vs. leadership varies depending on the role and the level, but I would say in very broad strokes that Amazon does 50/50 whereas Google does 90/10.\n",
    "\n",
    "    Neither company uses a numerical system to rank candidates, but I’ll use one for simplicity to illustrate my point. Let’s say Meets-the-Bar is ~7 out of 10. \n",
    "\n",
    "    Imagine Candidate A is 9 in Tech and 5 in Leadership. This would be a clear Google hire but likely not an Amazon hire. As an Amazon Bar Raiser, I would have a very hard time justifying a Hire on a candidate with 5 in Leadership, regardless of how amazing their coding skills were. They could be God's gift to mankind in coding, but with significant gaps in LPs, I don't think they would make Amazon a better place overall.\n",
    "\n",
    "    Imagine Candidate B is 6.8 out of 10 in Tech (~meets-the-bar with a few small concerns) and 9 out of 10 in Leadership. This candidate would likely not receive an offer from Google but may receive one from Amazon. As a Bar Raiser, I could be comfortable with some *small* concerns in Tech if the candidate exhibited bar raising Leadership Principles. For an SDE-I for example, Earn Trust, Learn and Be Curious, Ownership and Bias for Action would propel an engineer to very quickly improve their tech skills in the right environment. You can mentor trivial gaps in Tech much more easily than you can mentor gaps in Leadership.\n",
    "\n",
    "    A couple of years ago, I ended up having a spirited debate with a long-tenured senior engineer at Google about this very same thing. He made the shocking revelation that even when he was assigned “googlyness” as a competence, he still just asked the candidate a coding question, “because I get all the data I need from watching the candidate code.” -- No, you do NOT.\n",
    "\n",
    "    I've worked with amazing individuals on both companies, both in terms of Tech+IQ and Leadership. However different systems will inevitably generate different outputs given the same input.\n",
    "\n",
    "    Bottom line: would you rather work with Candidate A or with Candidate B? \n",
    "\n",
    "    This is why I work at Amazon.\n",
    "\n",
    "    (Yes that pic is Gilfoyle from the show Silicon Valley, one of the greatest shows for a nerd like me!)\n",
    "    \"\"\")\n",
    "\n",
    "comment = dedent(\"\"\"\n",
    "    I think the key difference is, if I had to assign probabilities\n",
    "\n",
    "    A) Probability that a \"leadership interview\" leads to a hire which is:\n",
    "    A good bullshitter: 90%\n",
    "    Competent: 20%\n",
    "    \"\"\")\n",
    "\n",
    "print(strip_code_fence(review_comment(post, comment)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"professionalism\": 0.7,\n",
      "    \"accuracy\": 0.6,\n",
      "    \"overall\": 0.65,\n",
      "    \"reason\": \"The comment maintains a level of professionalism while expressing disagreement and skepticism about the effectiveness of Amazon's interview process. However, it does not provide substantial data or evidence to support its claims, leading to slightly lower accuracy scores.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "comment = dedent(\"\"\"\n",
    "          While I used to be a believer on what you are advocating in your post, and also having done hundreds of interviews in Amazon I agree there's some signal about the candidate past behaviour in LP questions, but do you have any data to support that this is a good indicator of future performance and competence? how unbiased is the evaluation of LP questions, given that most of them are canned questions from the interview database and are reused again and again, also one can very easily prepare good answers for them, is also impossible to know how truthful the answers are, and past performance is not always the best indicator of future performance. If anything, from your post I'm starting to agree with the Google engineer who told you he gets all the data from watching the candidate code / do their job. You can also gather implicit data on how the candidate collaborates, responds to feedback, responds to new data or things they don't know, etc. Also your hypotethical scenario is flawed in terms of evaluating a person in 2 dimensions only, when we know people are much more rich and complex. Is challenging enough to evaluate the compentencies of a person during the limited glimpse and artificial scenario of a job interview.\n",
    "          \"\"\")\n",
    "\n",
    "print(strip_code_fence(review_comment(post, comment)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems like I was able to retrieve the necessary information from the contract using the `arco_contract_search` command.\n",
      "\n",
      "To summarize, according to the search results, the insurance coverage in contract XYZ-23 is as follows:\n",
      "\n",
      "* Coverage amount: up to $1,000,000\n",
      "* Duration: 2 years after the completion of the project\n",
      "\n",
      "This should provide a clear understanding of the insurance coverage included in the contract. If you need any further assistance or have more questions, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from textwrap import dedent\n",
    "\n",
    "# LITELLM_API_KEY = os.getenv('LITELLM_API_KEY', None)\n",
    "if not LITELLM_API_KEY:\n",
    "    raise Exception('Please set the LITELLM_API_KEY environment variable')\n",
    "\n",
    "def reply(messages, model='llama3.2:latest'):\n",
    "    url = \"https://litellm.dataturd.com/v1/chat/completions\"\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "    'Authorization': f'Bearer {LITELLM_API_KEY}'\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Request failed with status {response.status_code}: {response.text}\")\n",
    "    \n",
    "    return response.json()['choices'][0]['message']['content']\n",
    "\n",
    "def contract_question(question:str):\n",
    "    return reply([\n",
    "        {\"role\": \"system\", \"content\": dedent(f\"\"\"\n",
    "            You construction contract manager with access to a computer.\n",
    "            Use the following terminal commands to get additional information regardings contacts:\n",
    "            - `arco_contract_search $CONTACT_ID $QUERY`, use this to search for specify info within a contract.\n",
    "            \"\"\")},\n",
    "        {\"role\": \"user\", \"content\": dedent(f\"\"\"\n",
    "            Question: {question}\n",
    "            \"\"\")},\n",
    "        {\"role\": \"assistant\", \"content\": \"\"\"\n",
    "I can search for the specific information you're looking for. To find out how much insurance coverage is in contract XYZ-23, I'll use the following command:\n",
    "\n",
    "`arco_contract_search XYZ-23 \"insurance coverage\"`\n",
    "\n",
    "This should return the relevant details about the insurance coverage within contract XYZ-23.\n",
    "\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": dedent(f\"\"\"\n",
    "            line: 100\n",
    "            text: coverage up to $1,000,000 for 2 years after the completion of the project.\n",
    "            \"\"\")}\n",
    "    ])\n",
    "\n",
    "def strip_code_fence(text):\n",
    "    return '\\n'.join([line for line in text.split('\\n') if not line.startswith('```')])\n",
    "\n",
    "\n",
    "\n",
    "comment = dedent(\"\"\"\n",
    "    How much insurance coverage is in contract XYZ-23?\n",
    "    \"\"\")\n",
    "\n",
    "print(strip_code_fence(contract_question(comment)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
